# -*- coding: utf-8 -*-
"""Deep_Learning_Method_DISTILBERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qR6B9WEx7VP9GPsOev-NCz-bvaLiz4cd

**Deep Learning Approach with DestilBert**
"""

import torch
import pandas as pd
import numpy as np
import time
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch.optim as optim

!pip install scikit-learn

import torch
import pandas as pd
import numpy as np
import time

from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
import torch.optim as optim

"""**Importing Data**"""

df = pd.read_csv("/content/sample_data/dreaddit-train.csv")
X = df.drop('label', axis=1)
y = df['label']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=700, random_state=42)

df1 = pd.read_csv("/content/sample_data/dreaddit-test.csv")
X_test = df1.drop('label', axis=1)
y_test = df1['label']

import pandas as pd

# Load the train and validation dataset
df = pd.read_csv('/content/sample_data/dreaddit-train.csv')

# Select columns with 'LEX' prefix and the specific columns 'id' and 'subreddit'
lex_columns = [col for col in df.columns if col.startswith('lex_')]
# selected_columns = ['id', 'subreddit','label'] + lex_columns
selected_columns = ['id','label'] + lex_columns

# Create a new DataFrame with the filtered columns
filtered_df_train = df[selected_columns]

# Now 'filtered_df' has only the columns you specified
print(filtered_df_train)

X = filtered_df_train.drop('label', axis=1)
y = filtered_df_train['label']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=700, random_state=42)

# Load the test dataset
df = pd.read_csv("/content/sample_data/dreaddit-test.csv")

# Select columns with 'LEX' prefix and the specific columns 'id' and 'subreddit'
lex_columns = [col for col in df.columns if col.startswith('lex_')]
# selected_columns = ['id', 'subreddit','label'] + lex_columns
selected_columns = ['id','label'] + lex_columns


# Create a new DataFrame with the filtered columns
filtered_df_test = df[selected_columns]

# Now 'filtered_df' has only the columns you specified
print(filtered_df_test)

X_test = filtered_df_test.drop('label', axis=1)
y_test = filtered_df_test['label']

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

def create_model(input_shape):
    model = Sequential()
    model.add(Dense(128, activation='relu', input_shape=input_shape))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' for multi-class

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

model = create_model((X_train.shape[1],))

# Training with Validation
# Train the model using the training dataset and validate it on the validation dataset:

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluation
# Evaluate the model's performance on the test dataset:

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

# Performance Metrics
# Get predictions and calculate performance metrics:

from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5).astype(int)  # Adjust based on your output layer

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

print(f"Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {accuracy}")